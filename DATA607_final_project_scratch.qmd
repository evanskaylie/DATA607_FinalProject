---
title: "Final Project"
author: "Naomi Buell, Nick Kunze, and Kaylie Evans"
date: Sys.Date()
format: html
editor: visual
---

```{r}
#| label: load packages
#| message: false
library(tidyverse)
library(janitor)
library(jsonlite)
```

## Introduction

We signed up for the [Article Search API](https://developer.nytimes.com/docs/articlesearch-product/1/overview) from the New York Times. Below is an interface in R to read in the JSON data and transform it into an R data frame. We also attempt to merge this data with data from Letterboxd.

## Connect to NYT API

First, we sign up for an API key on the New York Times website. Here, we use the `rstudioapi::skForPassword()` function to keep my API key private when running the code in R Studio. For running and rendering all code for the purposes of this markdown, we also alternatively save the API key in an R chunk that we elect not to include in this published version.

The base HTTP request URL is defined below as well.

```{r}
#| label: set up API
#| error: true
#| message: false
#| warning: false
#| results: hide
api_key <- rstudioapi::askForPassword("Authorization Key")

base_url <-
  "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
```

```{r}
#| include: false
# Alternative to API key generated with the askForPassword function for the purposes of rendering my QMD
api_key <- "mC1y5Hr361gaqmvkjHpGd6WUdiL917vA"
```

Here, we create is a function to pull data based on the filter and page number parameters, starting from the newest articles.

Note that the Article Search API returns a max of 10 results at a time. We use the page query parameter to paginate through results (page=0 for results 1-10, page=1 for 11-20, etc. You can paginate through up to 100 pages (1,000 results)). Also note that we add a delay in the loop to avoid hitting the rate limit and getting a 429 error.

```{r}
#| label: create function
get_movies <- function(filter, num_pages, timeout) {
  # initialize data frame
  df <- tibble()
  
  for (page in seq_len(num_pages)) {
    # set url
    url <- paste0(base_url,
                  "fq=",
                  filter,
                  "sort=newest&page=",
                  page,
                  "&api-key=",
                  api_key)
    
    # initialize success as false before we get a success
    success <- FALSE
    
    while (success == FALSE) {
      # while success is false,
      tryCatch({
        df <- fromJSON(url, flatten = TRUE)$response$docs |>
          clean_names() |> rbind(df, .data) # append to df
        success <- TRUE # Set success to TRUE if no error occurs
      },
      
      # if error,
      error = function(e) {
        # Add a delay between requests to avoid hitting the rate limit
        Sys.sleep({timeout})
      })
    }
  }
  
  # Return the resulting data frame
  return(df)
}
```

## Load JSON NYT data into R data frame

We call the function, iterating through `num_pages` pages of JSON data, appending them together as the data frame `movies_df`. We will be using the Article Search API to get New York Times movie reviews, so we define the filter query accordingly.

```{r}
#| label: load data from API
#| message: false
#| warning: false

filter <-
  'section_name%3A%22Movies%22%20AND%20type_of_material%3A%22Review%22'
num_pages <- 100
num_seconds <- 9

nyt_movies_raw <- get_movies(filter, num_pages, num_seconds)
```

Here is the R data frame of NYT JSON movie review data loaded from the NYT API:

```{r}
#| label: browse df

head(nyt_movies_raw)
```

## Cleaning NYT data

Before we finalize this data set, we subset the data frame to our variables of interest and do some data transformation operations:

-   The `keywords` variable in the original data frame was a list-column. The `name` of the movie is tagged as a keyword in each review article. We unnest this list-column variable to extract the movie `name`.

-   Clean the movie `name` by

    -   converting all characters to lowercase

    -   removing parentheticals in the string for matching using REGEX

    -   removing any ", the" at the end of any movie names in the NYT data

    -   \[NOTE: In addition to pulling movie names from keyword tags, consider pulling them from other text? E.g., searching for the movie name within parentheses? From keywords, we lose about 120 out of 500 review rows that could be about movies (although some are about plays or other creative works).\]

-   Convert NYT publication date column `pub_date` to a datetime format.

```{r}
#| label: clean variables

# Define REGEX patterns
media_pattern <-
  ".*\\(([^\\(\\)]+)\\)$" # to get media type (e.g., "movie", "play", etc.), pull text in final parenthetical of movie title
name_pattern <-
  "(.*) \\(([^\\(\\)]+)\\)$" # to get movie title on it's own, remove final parenthetical
the_pattern <- "^the\\s" # to remove "the" from beginning of movie title
comma_the_pattern <- ",\\sthe$" # to remove ", the" from end of movie title
a_pattern <- "^a\\s" # to remove "a" from beginning of movie title
comma_a_pattern <- ",\\sa$" # to remove ", a" from end of movie title

nyt_movies <-  nyt_movies_raw |>
  # Unnest `keywords` list variable
  unnest(keywords, keep_empty = TRUE) |>
  # Correct date column format
  mutate(
    pub_date = as_datetime(pub_date),
    media = str_match(value, media_pattern)[, 2] |>
      tolower(),
    # Clean movie titles, removing "the"
    name = str_match(value, name_pattern)[, 2] |>
      tolower() |>
      str_replace(comma_the_pattern, "") |>
      str_replace(the_pattern, "") |>
      str_replace(a_pattern, "") |>
      str_replace(comma_a_pattern, "")
  ) |>
  # Filter data to movies only (removing keyword rows for crew, other types of media, etc.)
  filter(media == "movie") |>
  # Reorder reviews by publication date
  arrange(desc(pub_date)) |>
  # Select variables of interest: columns w/ text for sentiment analysis and merging
  select(
    abstract,
    lead_paragraph,
    name,
    pub_date,
    headline_main,
    headline_kicker,
    headline_print_headline
  )

head(nyt_movies)
```

Above is our merged data set.

## Load Letterboxd data and clean

Load Letterboxd data and clean variables. We perform the same cleaning transofrmations to the movie `name` variable that we did with the variable of the same name in the NYT data above. Also, we drop movies released before the earliest review in the NYT data (`r min(nyt_movies$pub_date)`); we don't want to merge these older Letterboxd movies since it's unlikely that an NYT critic would review a movie more than a year after its release.

```{r}
#| label: load letterboxd
  
# load letterboxd data
letterboxd_movies <-
  read_csv(
    "https://raw.githubusercontent.com/naomibuell/DATA607_FinalProject/main/movies_trimmed.csv"
  ) |>
  drop_na(minute) |>
  mutate(
    name = tolower(name) |> # switching movie names to lower case
      # Use str_match() with the pattern
      str_replace(comma_the_pattern, "") |>
      str_replace(the_pattern, "") |>
      str_replace(a_pattern, "") |>
      str_replace(comma_a_pattern, "")
  ) |>
  select(-c(id)) |> 
  # only keep Letterboxd data within a similar date range to the NYT data
  filter(date >= year(min(nyt_movies$pub_date)) - 1)

head(letterboxd_movies)
```

## Merge NYT and Letterboxd Data

Below, we merge this NYT data with Letterboxd data based on movie `name`. We also pick the best matches based on when the NYT critic reviewed the movie and when Letterboxd says the movie was released, assuming a true match would show the movie reviewed by NYT right when it came out (since Letterboxd only posts the year of release and not the full date, we assume all movies came out on Christmas day for the purposes of this calculation).

```{r}
#| label: merge Letterboxd and NYT
#| warning: false
#| message: false

# First, merge datasets by name. This is a many to many join, so movie-review rows will not be unique, with some extra matches done in error.
merged <- inner_join(nyt_movies, letterboxd_movies) |>
  
  # For movies w/ multiple matched rows, choose absolute difference in dates between sources.
  group_by(name) |>
  mutate(
    # assuming all movies came out on Christmas (popular date for movie release),
    release_date = as_datetime(paste0(date, "-12-25")),
    # calculate absolute diff btwn review and release date
    dates_diff = abs(difftime(pub_date, release_date))
  ) |> 
  filter(dates_diff == min(dates_diff)) |> # for each unique name, get movie row w/ shortest diff between review and release date
  ungroup() |>
  # remove any duplicated rows
  distinct()

head(merged)
```

## Statistical Analysis and Graphic to Describe or Validate Data

Now that we have our analysis data set, we can perform a few statistical analyses to describe and validate the merged data.

### Descriptive statistics

Identify discrepancies, check for consistency and completeness. Check for outliers or anomalies in the merged data. Look for consistent data types and formats across the merged data. Look for unexpected patterns that may indicate merge errors.

First, we compare the distribution of dates reported by both data sets:

```{r}
#| label: numeric descriptive stats: dates
#| message: false
#| warning: false

# compare dates for NYT and Letterboxd data
merged |>
  ggplot() +
  geom_histogram(aes(x = pub_date, fill = "Review publication date (source: NYT)"),
                 alpha = .5, bins =  44) +
  geom_histogram(aes(x = release_date, fill = "Movie release year (source: Letterbox)"),
                 alpha = .5, bins = 44) +
  theme(legend.position = "bottom") +
  labs(title = "Dates, by data source",
       x = "Date",
       y = "Frequency")
```

Both movie release and review publication dates align well between data sources.

```{r}
# check movie duration (mins) for outliers

# Check ratings for outliers

# create boxplot of each to investigate outliers
```

Plot histograms or box plots of key variables from the merged data and original sources. Look for shifts in the distribution that may indicate merge errors. If you have time-series data, plot trends from the original sources and the merged data to see if there are any unexpected discrepancies.

```{r}

```

### **Data Quality Assessment/Overlap Analysis**

Check the number of records and variables from each source that were merged. Identify any missing or duplicate records.

```{r}
perc_merged <- nrow(merged)/nrow(nyt_movies)

```

`r round(perc_merged)*100` percent of data from NYT was able to be merged with a Letterboxd rating!

```{r}
#| label: check duplicate movie rows

duplicates <- merged |>
  group_by(name) |>
  mutate(duplicate = n() > 1) |>
  filter(duplicate)

num_duplicated_rows <- duplicates |>
  nrow()/2
```

There are only `r duplicate_rows` duplicated movie rows, where a movie title matched to more than one observation in either the Letterboxd or the NYT databases. We can manually review to determine which observation is a true match, then drop the other observation(s).

```{r}
#| label: manually remove duplicate rows



```

## Sentiment Analysis of NYT Review using Cloud-based Azure Service

Insert sentiment analysis here:

```{r}
#| label: sentiment analysis


```

Create graphic:

```{r}
#| label: conclusion graphic
```

## Conclusion

\[Add conclusion text here\]
